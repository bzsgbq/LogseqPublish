public:: true

- ## Todos
	- LATER 买牙刷
	- LATER 博士青托报名: https://mp.weixin.qq.com/s/_Ey4AuttkJWO-LSjeRE5MQ
	  :LOGBOOK:
	  CLOCK: [2025-11-05 Wed 13:40:34]--[2025-11-05 Wed 13:40:34] =>  00:00:00
	  :END:
- ## Chats
  collapsed:: true
	- > qinqin: 
	  "但是能回家的话肯定生活过得舒服好多"
	-
## Papers
	- ### 2025-11-04
		- #consciousness [意识科学中的范畴论：超越“相关性工程”](https://mp.weixin.qq.com/s/mOp9i-DABiMt-8t0nz4m-w) 感觉挺牛挺有意思的一篇文章, 对于人类理解解释意识, 有积极的促进作用
		  collapsed:: true
			- ### 一句话总结（核心结论）
				- 作者主张：单纯做“脑-意识相关性”的工作（即找神经相关物/NCC）不够，要把“关系/结构”这类抽象层次纳入研究。
				- **范畴论（category theory）**——一种研究“对象之间关系和模式”的数学语言——可以作为桥梁，把大脑的数学结构和意识的“现象/关系结构”系统地联系起来，从而推进理论整合、解释力提升，甚至为意识与人工智能的关系提供新视角。
			- ### 论文为什么要提出这个主张（动机）
				- 近几十年意识科学很多研究都做“相关性工程”——找神经活动模式与意识经验之间的关联（NCC）。这种做法方法上成功，但**并没有真正回答“为什么”或“如何”**把心理和物理联起来（也就是仍然面对“难题”）。
				- 作者认为，需要一种抽象的、与形而上学中立的工具来表述“结构”和“关系”，从而更系统地比较、整合不同理论并把问题在两个领域间转换。范畴论就是这样一把“抽象但精确”的工具。
			- ### 关键概念浅显解释（带例子）
				- **范畴（category）**：把 "研究对象" 和 "对象之间的 '箭头/变换'（morphism）" 放到一起看，并要求箭头可以复合（composition）、符合结合律、每个对象有恒等箭头。
					- 比喻：把烘焙一整个过程看成“范畴”，原材料是对象，把“把蛋打发→加入糖→烘烤”看成箭头，它们可以按顺序拼合成完整过程。
				- **函子（functor）**：把一个范畴里的对象与箭头系统性地映射到另一个范畴，同时保留“复合”和“恒等”的结构。
					- 比喻：把地图（几何对象）转换成数字表格（代数对象）的过程，如果转换保留了足够结构，那么在代数域证明的事实可以“反推回”几何域（论文用 Brouwer 不动点定理举例说明这种跨域转移的价值）。
				- **为什么范畴论对意识有用？**
					- 因为意识问题很多是“结构/关系”问题（例如不同体验间的相似性、体验如何“构成”复杂场景等）。范畴论的天赋就是表达“关系之上还有关系”的结构（甚至能表达“关系的关系”）。
			- ### 作者如何把已有理论（以 IIT 为例）放进范畴论框架？
				- IIT（Integrated Information Theory）把系统的“因果-效应结构”与体验质量联系起来（IIT 的“中心身份”说体验在某种程度等同于系统的某个数学结构）。
				- 作者与已有研究（Tsuchiya 等人、Kleiner & Tull 等）讨论了：我们可以把**“大脑一侧”**的结构和**“意识一侧”**的结构每一边都抽象成一个范畴（比如把 IIT 的因果结构当成大脑范畴里的对象/箭头）。然后探讨是否存在把“脑范畴”映到“意识范畴”的函子（或更强的等价/同构关系）。
				- 重要区分：**有函子**只是说明两个域之间有系统性的对应，但并不等同于“同一”或“本体上的等价”。论文强调不能把函子映射直接当成“解决难题”的终点，而是把它当成一种把难问题从一个域转移到另一个域的有力工具。
			- ### 论文提出的具体“好处/用途”——三条主线
				- **更严谨地理解并测试 NCC（从相关到结构化）**：通过把脑与意识都写成范畴，检验是否存在保结构的映射（例如“同样刺激不同体验”或“不同刺激同样体验”等实验情形可以用于检验映射是否合理）。这能把“相关性”问题变为“结构保持/破坏”的具体可测问题。
				- **理论整合**：范畴论可以作为“通用语言”把不同派别（IIT、时空理论、意识代理理论等）放入同一框架，找出彼此的共同点与差异，从而促进互通与整合，而不是各说各话。
				- **利用“对偶/对称”（duality）做解释转换**：像物理学里用 AdS/CFT 之类的对偶把难题从一个域转到更易处理的域；在意识科学中，或许可以把某些难做的神经计算（例如 IIT 中的组合最优划分问题）转译为更简单的现象学/结构问题来解。论文把这种思路作为值得探索的高回报方向，但也承认目前仍是推测。
			- ### 作者的谨慎与局限
				- 范畴论很抽象，能把几乎任何东西形式化（这既是优点也是风险）：若不小心，可能“什么都能拟合”，但没有实际可检验的结论。作者强调必须把范畴化和经验数据/现象学严格连接，尤其需要更好地把“现象学（第一人称）”形式化为合适的范畴结构。
				- 论文并不声称已经解决“难问题”，而是提出范畴论作为一种**有潜力的研究路线**，并呼吁把这条路细化、对接具体模型与数据。
			- ### 小结（对非专业读者的口语版）
				- 想象研究意识像在拼两套乐高：一套是“脑的结构拼图”，另一套是“意识体验的结构拼图”。到目前为止，科学家多做的是把两套拼图的某些块放在一起看它们是否同时亮（相关）。本文建议用范畴论当作说明书，不只是看“哪些块同时亮”，而是研究“拼接方式和规则”——如果能把两套拼接规则系统地对应起来（即找到稳定的映射/转换），我们就能把一些在体验域很难回答的问题，变成在脑域更可操作的问题，反之亦然。这样既能促进理论之间的沟通，也可能带来新的可测预测。
		- #neuroscience-inspired [AI 的 foundation models 会改变脑科学吗？](https://mp.weixin.qq.com/s/DJkbTOZuVjI_kBWHaP9Tgg) 并不是很赞同这篇文章捏
		  collapsed:: true
			- ### 一句话概览
				- 作者讨论了“大型预训练模型（foundation models）”能在多大程度上推动脑科学的发展——这些模型在**预测**上很强，但能不能带来**真正的理解（mechanistic explanation）**，还存在重大问题与机会。
			- ### 1）什么是“foundation model”（基础模型）？为啥重要？
				- 简单说，基础模型就是先在海量、无标签的数据上用自监督学习（比如预测下一个词）训练出的巨型网络，然后再针对具体任务做微调（finetune）。它的优势是能从原始数据自动学出“通用表征”，再用很简单的方法（比如线性分类器）解决很多下游问题。这个套路在语言、图像、语音、蛋白质结构等领域都很成功。论文把这种方法的基本流程和原理解释得很清楚。
				- 举个类比：以前做任务要人工设计很多“特征”（像手工做工具），现在是先造一台多能机器（预训练），再用很少的调试去完成不同任务（微调）。
			- ### 2）论文里提到的两个具体例子：脑活动与行为的“foundation models”
				- 作者举了两个近期亮点案例：
					- 一个用大量**小鼠视觉皮层的钙成像（calcium imaging）**数据训练的模型，能预测神经元对新刺激的响应，还能反映某些解剖学信息（比如细胞类型、树突形态、连接性）。
					- 一个叫 **Centaur** 的行为基础模型，训练目标是预测人在很多心理学实验中的决策（把任务描述和以往选择当作“序列”来预测下一步选择），在某些预测任务上超过传统认知模型。
				- 这些结果让人兴奋，因为模型能跨任务、跨被试做出合理预测——看上去好像能“模拟”大脑或人类行为。
			- ### 3）但“预测好”不等于“理解深”
				- 这是论文的核心论点之一：**预测 ≠ 解释（Prediction is not Explanation）**。作者用历史和实验例子说明：即便一个模型能非常准确地预测结果，也不意味着它学到了生成这些结果的真实机制。举两个直观的比喻和例子：
					- 比喻：托勒密的天体表（本轮本圈）能很好预测行星位置，但不是真正的行星运动原理。模型可能像“表”而非“原因”。
					- 例子：有的模型在棋类或轨道运动等合成任务上表现很好，但在稍微不同的相关任务上就崩溃了，说明它没学到真正的“规则”，可能只是习得了训练数据里的统计技巧。
				- 论文特别指出：像 Centaur 有时候能在没有任务信息的情况下仅靠统计规律给出准确预测，这种策略可能和人类真实的决策机制完全不一样。也就是说，模型“表现像”人，但内部机制可能截然不同。
			- ### 4）那怎样能把“黑盒预测器”变成“有科学价值的解释器”？
				- 作者提出了若干方向和方法，核心思想是**把这些模型的计算结果和神经、心理学已有理论联系起来，并用可检验的实验去验证**。具体包括：
					- **机械可解释性（mechanistic interpretability）**：把大型网络内部拆成更小的“子电路”或“原语操作”（比如某些注意力头、隐层单元承担特定功能），找出可重复出现的计算模块。这像把复杂机器拆成零件，看看每个零件在做什么。作者认为这类研究已经开始取得进展（能在模型里找到执行特定步骤的“电路”）。
					- **解剖级、表征级连接**：在表征（representation）层面，有研究发现模型里能找到“类似概念的向量方向”（比如把“英语→西班牙语”看成一个向量），像这样的线性结构可以导致可检验的大脑假设——例如：如果人类的大脑也以类似线性方式组织语义关系，那么我们应该能用线性运算从神经活动中恢复相应概念。作者把这种“线性代数假设”列为可实验检验的预测。
					- **引入生物学约束**：单纯放大模型和数据不一定会得到真实的生物机制，因为生物的表征能力受进化和发育过程的路径依赖限制。把“进化/发育式”的约束嵌入模型（例如特定的网络结构、学习规则、训练课程）可能更有利于得到与脑机制相近的结果。
				- 总之，作者主张不要只看“预测分数”，而要把模型内部结构、产生预测的方式、与已有神经/心理理论的对应关系，都纳入评估。
			- ### 5）局限与机会（总结性看法）
				- **局限**：目前的基础模型往往追求规模和预测性能，但这不保证它们学到了与生物真实机制同源的东西（路径依赖和进化开发差异会造成偏差）。另外，即使模型很擅长拟合数据，我们也可能把“大脑黑盒”换成另一个“黑盒”。
				- **机会**：即便机制不同，研究这些模型的内部计算也能给出**可检验的理论（testable hypotheses）**，推动实验设计、激发新理论、甚至发现新的计算原语，这对神经科学本身是有价值的。作者把模型作为“理论-实验”的催化剂来看待：通过解释模型、提出假设、再回到实验验证，形成反馈循环。
			- ### 6）结论（作者的态度）
				- 作者并不认为基础模型会自动“拯救”神经科学；关键在于**把这些模型从单纯的数据拟合工具，转化为能提出并检验机制性假设的科学工具**。也就是说，要把“好预测”变成“有解释力”的理论。否则我们只是把一个黑盒换成另一个黑盒。论文最后强调：未来能否利用基础模型真正推进理解大脑，取决于我们能否将其计算结构与脑科学的理论和实验紧密结合。
			- ### 附：对非专业读者的快速FAQ
				- Q：这是不是说 AI 模型不能用来做脑科学？
					- A：不是。作者认为 AI 模型很有用，但**有益处的关键是把模型解释清楚并与实验结合**，而不是仅仅拿高预测精度当成理解大脑的证据。
				- Q：作为普通人，什么能直观理解“预测 vs 解释”的差别？
					- A：想象会下雨。气象台的模型能准确预测“明天下雨”，这是预测。但如果你想知道“为什么这次会下雨”（例如某个海温异常、气流环路），那就是解释。两者都重要，但提供解释需要不同的信息与分析方法。
		- #agent #memory [==NeurIPS 2025 Spotlight | NUS 等提出 G-Memory：让多智能体拥有“组织级记忆”，实现自我进化==](https://mp.weixin.qq.com/s/Wkct7mXzJcKx4XYnSdMKYw) 与我们的研究方向最相关, 可以重点看下
		  collapsed:: true
			- ### 1) 为什么要做这项工作？（问题与动机，用生活类比）
				- 想象一个团队去做一件事（比如装修一间房）：每个人负责不同工作（水电、油漆、采购），他们之间有很多对话、分配、纠错。单次任务结束后，团队会积累经验：哪些步骤容易出错、哪些做法更高效。对于人类团队，我们会把这些经验写入团队手册或总结，下一次做类似任务就能更快、更少犯错。
				- 现在把“团队”换成用大模型（LLM）驱动的**多智能体系统（MAS）**——有很多“智能体”互相聊天、分工、执行。论文指出：目前的多智能体系统**缺少像人类组织那样的系统化记忆**。已有的一些“记忆”很粗糙（例如只保存最后结果或简单日志），不能有效地把跨次任务的经验利用起来，使得系统难以“自我进化”。且多智能体的交互上下文比单个智能体复杂得多（上下文长度可达单智能体的数倍），直接把历史对话原样丢回去并不能帮助决策，反而会造成信息过载。
			- ### 2) G-Memory 的核心想法（一句话版）
				- 把多智能体的长期历史按**三层有结构的图（graph）**组织起来：
					- **Insight（结论/规律）图 → Query（任务/案例）图 → Interaction（对话/轨迹）图**，
				- 并在新任务到来时做“双向检索”：
					- 向上找“通用的经验/教训”，
					- 向下找“精简的具体协作轨迹”，
					- 然后把**按角色定制**地喂给每个智能体，从而帮助它们更好地分工与执行。
				- 用类比：
					- Insight 是“团队手册里的经验条目”，
					- Query 是“之前做过的具体项目记录（目录）”，
					- Interaction 是“当时成员的聊天记录与操作步骤”。
					- 当来新任务时，先在目录里找相似项目，再从目录跳到手册摘取相关经验，同时把历史聊天抽取出关键步骤并压缩给团队成员看。
			- ### 3) 三层图结构具体是啥？（逐层解释）
				- **Interaction Graph（交互图 / Utterance Graph）**
					- 最底层，记录某一次任务全过程中每一句话、每个动作（节点是单条“说话/动作”，边表示“哪一句话启发了下一句”）。这是最细粒度的轨迹数据。
				- **Query Graph（任务/案例图）**
					- 中间层，把每一次处理的“查询/任务”当成节点（节点里包含任务描述、执行结果、对应的 interaction graph）。节点间有边，表示两个任务之间语义或经验上的关联（比如“任务A的做法对任务B有参考价值”）。这个图可以用来检索与当前任务语义相关的历史任务，而不是只靠简单向量相似度。
				- **Insight Graph（洞见/策略图）**
					- 顶层，把从多个任务里浓缩出的**可迁移的高层经验、策略、注意事项**作为节点（每个 insight 会记录哪些任务支持它）。Insight 之间也有带上下文的关联（有点像“经验之间的因果/引用关系”）。
			- ### 4) 新任务到来时 G-Memory 怎么工作？（四步流程，用示例说明）
				- **整体思路**：
					- 先粗筛相关历史，再“向上”和“向下”两头检索，最后把经过筛选和压缩的信息分发给每个角色的智能体，任务完了再把新经验写回三层图中，循环演进。
				- 详细如下：
					- **步骤 A — 粗检索（先找“目录”）**
						- 用语义向量（例如 MiniLM）对 query_graph 做 top-k 相似度检索，得到 k 条最相关的历史任务（QS），再把 QS 的 1-hop 邻居一并拿进来（hop expansion），以免漏掉图结构上的有用关联。这样既用语义也用图结构扩大检索。
					- **步骤 B — 向上（Query → Insight）**
						- 把检索到的相关任务映射到 Insight 图，收集所有与这些相关任务相关联的高级“经验/教训”。这些是能跨任务通用的策略（例如“在做网页搜索时先确认实体的全名以避免歧义”）。这个过程叫 upward traversal。
					- **步骤 C — 向下（Query → Interaction）**
						- 同时对检索到的任务的 interaction_graph 做 **图稀疏/压缩（SLLM）**：利用 LLM 去抽取“核心协作轨迹（核心对话与步骤）”，把冗余对话删除，只保留对当前任务实质有帮助的步骤（compressed trajectory）。这就是 downward traversal，用来给当前团队提供“可操作的具体做法”。（论文中把这个压缩过程实现为 prompts + LLM 评分与抽取模块。）
					- **步骤 D — 角色定制与注入（把记忆喂给每个成员）**
						- 不同智能体在系统中扮演不同角色（如 Solver、Ground Truth、Executor）。G-Memory 有一个函数 Φ 会把上面得到的 insights 和压缩轨迹**按角色筛选/个性化**（用 prompt-based rewriting）成每个智能体的内部记忆片段，然后这些智能体在接下来的交互中使用这些记忆。举例：执行者只需要“具体步骤与注意事项”，而验证者需要“常见失败模式与核查清单”。
				- **示例（论文中的 case）**：
					- 任务是“把一个干净的布放在台面上（put a clean cloth in countertop）”。G-Memory 找到历史上“把干净的蛋放到微波炉里”的类似案例（语义相似），并从历史对话中提取到“先清洁再放置”的关键步骤（以及历史中出错的分工情形）。这些高层 insight（“先确保物品已清洁”）和压缩轨迹（谁去清洁、谁去放置）被分别注入给 Ground Agent 与 Executor，结果提高了执行成功率。
			- ### 5) 做完任务后如何把新经验写回去？（学习与进化）
				- 任务执行完毕并得到环境反馈（成功/失败、token 成本等）后，G-Memory 会同时更新三层图：
				- **把完整的对话/轨迹存入 Interaction Graph**（记录原始材料）；
				- **把新任务作为节点加入 Query Graph**，并与检索到的历史任务与支撑 insights 做连边（建立索引与关联）；
				- **生成新的 insight（如果有）并把它加入 Insight Graph，同时把它与此前用到的 insights 建边**。
			- ### 6) 论文里的关键实现与超参数（总结）
				- Embedding 用 MiniLM（ALL-MINILM-L6-V2）。检索时 k（检索的 top-k）通常取 1 或 2，hop expansion 以 1-hop 为优（论文实验表明 2-hop/更大会引入噪声）。论文还用一个 LLM 基的相关性评分 RLLM 和稀疏器 SLLM 来挑选核心交互轨迹。
				- 在角色定制里，用 prompt 模板把“通用 insight”变成“角色化建议”。论文把这些 prompt 也写了出来（附录里有具体 prompt）。
			- ### 7) 实验：G-Memory 有多好？（结论与定量效果，通俗讲）
				- 论文在 5 个 benchmark（两类知识推理 HotpotQA / FEVER；两类实体动作 SciWorld / ALFWorld；以及 PDDL 游戏）上把 G-Memory 插到三种主流 MAS 框架（AutoGen、DyLAN、MacNet）并与若干 baseline（单智能体记忆设计和其他 MAS 记忆）比较。
				- 主要结论：
					- **性能提升显著**：在多项设置里，G-Memory 能把基线提升若干个百分点――比如在 ALFWorld 上在某些配置下相比最好的 baseline 提升可达 **约 20.89%**；在知识 QA 任务上也能提升约 **10.12%**。这些数字说明：有结构化的、多层次、按角色定制的记忆对 MAS 非常有效。
					- **代价（token 成本）适中**：相比一些把所有历史上下文直接喂回去的做法，G-Memory 在 token 开销上更节省或仅略增（因为它会压缩轨迹、只取关键案例），所以是“性能/成本”比较划算的方案。论文里用图展示了不同方法的性能 vs token 成本的折中。
					- **消融研究**：去掉高层 insight 或去掉压缩的 interaction 都会降性能，说明“高层经验”和“具体协作轨迹”两者都重要，interaction（细粒度轨迹）对性能影响略大一些。超参数（如 hop 数、k）对效果敏感，论文建议通常用 1-hop、k ∈ {1,2}。
			- ### 8) 为什么这种方法有效？（直观解释）
				- **既有抽象又有具体**：高层 insight 相当于把“可迁移的经验”抽出来，帮助系统避免重复犯错；而压缩的 interaction 提供了可直接复制的“操作模式”（谁做什么、怎么做），两者结合既能泛化也能落地。
				- **按角色定制**：不同智能体需要的信息不同（像团队中不同工种），G-Memory 把记忆按角色过滤/改写，使得每个智能体拿到的是“正合适”的信息，而非一锅烩。
				- **利用图拓扑而非单纯向量相似度**：Query Graph 的网络结构能发现跨任务的间接关联（图上邻居），避免只靠局部相似度错过有用案例。
			- ### 9) 局限与未来方向（论文也提到的）
				- 论文主要在五个 benchmark 上验证，**尚未覆盖一些高风险或专门领域**（例如医疗 QA），需要进一步验证可推广性。
				- G-Memory 本身依赖 LLM 对稀疏与重写的能力（SLLM、RLLM、prompt），因此如果底层 LLM 有偏差或被对抗，记忆可能会放大错误（论文在影响声明中也有提示需谨慎部署）。
			- ### 10) 快速复述（3句话总结）
				- 多智能体系统缺乏能跨任务、按角色学习的记忆；简单地把历史对话全部回放效果很差。
				- G-Memory 用三层图（Insight / Query / Interaction）组织经验，并在新任务时做“向上找经验、向下找轨迹”的双向检索，再按角色定制注入给各个智能体。
				- 在多种基准和框架上，G-Memory 能显著提高成功率与准确率，同时保持代价可控，是让 MAS 能“逐步自我进化”的有效方案。
		- #psychology [为什么我朋友很多，但还是会感到孤独？Nature子刊研究揭示“掌控感”的秘密](https://mp.weixin.qq.com/s/oHNzoPy2iZJClqesT8j-tA) 纯心理学研究, 当小品文看挺有意思, 但对我们的实际科研没有直接帮助~
		  collapsed:: true
			- 研究指出，孤独感是个体在两个基本需求维度上状态的反映，这两个维度分别是 ：
				- **社交联系 (Social Connection / Communion)**: 即关系维度，反映了“我们” (we) 的需求。它代表个体从社会关系中获得的关怀、支持和归属感 。
				- **个人能动性 (Personal Agency)**: 即个体维度，反映了“我” (me) 的需求。它涵盖了自我导向 (self-direction)、选择 (choice) 和对生活的个人控制感 (personal control) 。
			- **核心发现一**
				- 即便你拥有足够多的社会支持（高 Communion），但如果你在关系中感到无法“做自己”、缺乏控制感（低 Agency），你依然会经历中等程度的孤独。**这完美解答了“为何人群中的孤独”如此普遍。
			- **核心发现二：纵向转变 (Longitudinal Transitions)**
				- 研究进一步利用纵向数据追踪了个体在不同原型间的“转变” (transitions) :
					- **孤独感增加**： 当个体向“更不利”的原型转变时（例如，从“赋能型”转变为“沉默型”），他们的孤独感水平会随之显著增加 。
					- **孤独感减少**： 当个体向“更有利”的原型转变时（例如，从“受忽视型”转向“分离型”或“沉默型”），他们的孤独感水平则会显著降低 。
			- **结论**
				- 这项研究将孤独感的理解从“关系赤字”的一维视角，拓展到了“社会联系”与“个人能动性”协同作用的二维平面 。**孤独不是一种简单的赤字，而是一种动态的“配置”失衡。**
				- **这对孤独干预 (loneliness interventions) 提出了全新的思路：我们不仅要帮助人们建立连接，更要关注他们的个人能动性。**
					- 对于**“分离型”**的人，干预重点是关系建立 (relationship-building)。
					- 对于**“沉默型”**的人，干预重点则应是自主训练 (autonomy training)，例如帮助他们设定人际边界或练习自我需求倡导 (self-advocacy) 。
					- 对于**“受忽视型”**的人，则需要双管齐下的综合支持 。
		- #neuroscience #soul [通过神经相似性预测陌生人是否会成为朋友](https://www.nature.com/articles/s41562-025-02266-7)
		  collapsed:: true
			- 这篇发表在《自然·人类行为》上的论文，核心思想非常有趣，它试图回答一个我们每个人都可能好奇的问题：**两个人能否成为朋友，是不是在他们相遇之前就已经由大脑“注定”了？**
			- ### 核心比喻：大脑的“电影观后感”
				- 想象一下，你和一群陌生人一起走进一个电影院，观看一部混合了纪录片、喜剧、辩论等不同风格片段的电影合集。看完后，科学家们不是问你们“喜欢哪个片段”（这是传统的问卷调查），而是直接去测量你们每个人的**大脑在看电影时的实时活动**。
				- 这项研究的基本假设是：**你们大脑活动的“波形”越相似，说明你们对电影的理解、关注点和情感反应越一致，即你们看待世界的方式越像。** 这种深层次的相似性，可能会让你们在未来更容易成为朋友。
			- ### 研究是如何进行的？（三步走）
				- 为了验证这个想法，研究者设计了一个非常巧妙的纵向研究（即跟踪一段时间）：
				- **第一步：测量“陌生人”的大脑（时间点1）**
					- **参与者：** 一所大学里41名刚入学、彼此还不认识的研究生。
					- **做什么：** 在他们开学前或刚开学几天内（尽量确保他们还没成为朋友），用功能性磁共振成像（fMRI）扫描他们的大脑，同时让他们观看一系列视频片段。
					- **得到什么：** 获得了每个参与者大脑多个区域在观看视频时的活动数据。然后，计算任意两个人之间大脑活动的相似度（即“神经相似性”）。
				- **第二步：绘制社交网络图（时间点2和3）**
					- **时间点2（开学后2个月）：** 对整个年级的近300名学生进行问卷调查，画出第一张“友谊地图”。谁和谁互相认为是朋友，他们之间的距离就是“1”。通过共同朋友认识的，距离就是“2”，以此类推。
					- **时间点3（开学后8个月）：** 再次进行同样的调查，画出第二张“友谊地图”。这时，一些人的关系变得更近了（距离减小），一些人疏远了（距离增加），还有一些人关系没变。
				- **第三步：关联分析**
					- 核心问题来了：把第一步得到的 **“陌生人时期的大脑相似度”** ，和第三步得到的 **“8个月后的友谊距离”** 以及 **“关系是变好还是变坏”** 放在一起看，它们之间有联系吗？
			- ### 发现了什么？（主要结论）
				- 答案是：**有！**
				- 1.  **预测谁会成为朋友：**
					- 研究发现，在8个月后成为**直接朋友（距离为1）** 的两个人，在他们还是陌生人时，大脑中一个叫**左侧眶额叶皮层**的区域的活动，就比那些最终在社交网络上隔得很远（距离为3）的人要相似。
					- **通俗理解：** 这个脑区和处理“主观价值”有关（比如觉得什么有趣、什么值得关注）。这说明，未来能成为朋友的人，可能早在相遇前，就对事物有着相似的品味和偏好。
				- 2.  **更强大的预测：关系的变化**
					- 一个更显著的发现是，**神经相似性能够预测两个人关系的“走势”**。
					- 与那些在6个月内关系**疏远**了的人相比，那些关系**变得更紧密**的人，在他们还是陌生人时，大脑活动就表现出更高的相似性。而且，这种相似性遍布**超过40个大脑区域**，包括：
						- **默认模式网络：** 负责理解他人想法、产生共情、思考人生意义（“走神”网络）。你们对这个世界的“脑补”方式越像，越容易走近。
						- **额顶控制网络：** 负责控制注意力、进行复杂思考。这说明你们分配注意力和思考问题的方式可能很同步。
						- **背侧注意网络：** 负责对外界刺激分配注意力。你们可能自然而然地会被视频中相同的东西所吸引。
			- ### 重要补充和深入解读
				- **这不仅仅是“兴趣相投”**
					- 研究者担心，大脑相似只是因为两个人都同样“喜欢”或“觉得”视频有趣。于是他们控制了参与者对视频的“兴趣度”和“享受度”评分，发现结果依然成立。
					- **这说明：** 神经相似性捕捉到的东西，比我们口头能说出来的“喜欢”要更深层，它反映了我们无意识的、内在的认知和处理世界的方式。
				- **和“物以类聚”的关系？**
					- 我们都知道，年龄、性别、背景相似的人更容易成为朋友（这叫“同质性”）。研究发现，在预测“直接朋友 vs. 远距离熟人”时，**性别相似性**等社会人口学因素确实部分解释了大脑的相似性。
					- **但是！** 在预测“关系是会变好还是会变坏”时，即使考虑了所有这些外在因素，**神经相似性的预测力依然非常强大**。
					- **通俗解读：** 有些初期的友谊可能源于“就近原则”（比如恰好是室友或同桌），这种友谊可能因为外在条件而快速形成，但也可能因为缺乏深层次的契合而慢慢消散。而那些能够持续深化、变得紧密的友谊，其根基更可能是这种由神经相似性所代表的、深层次的“灵魂契合”。
			- ### 总结与启示
				- **核心结论：**
					- 在我们相遇之前，我们大脑处理外界信息的方式（即我们如何“感受”和“理解”这个世界），已经悄悄埋下了友谊的种子。这种深层次的相似性，不仅能预测谁会成为朋友，更能预测哪些关系会历久弥新。
				- **一个生动的场景：**
					- 想象开学初，你可能因为座位近和A成了朋友，也因为都喜欢打篮球和B成了朋友。但一学期下来，你和A可能慢慢淡了，而和B却无话不谈。这项研究告诉我们，也许在你和B一起看电影之前，你们的大脑就已经是“同频共振”的状态了。
				- **局限与未来：**
					- 这项研究是在一个特定人群（商学院研究生）中进行的，结论能否推广到所有文化和环境还需验证。而且，它主要证明了“神经同质性”（相似的人相互吸引）的存在，但成为朋友后，彼此的相互影响（社会影响）也会让大脑变得更像，这是一个相互作用的过程。
		- #context [Context Engineering 2.0：在未来，一个人的本质，就是其所有上下文的总和｜上海交大](https://mp.weixin.qq.com/s/bXkKcwAyiAw8A0KIP1fRdw) 很科幻很上层lol
		  collapsed:: true
			- qinqin
				- 神奇的文字，有一些用ai编程的小技巧和哲学思考lol（人可以被总结成上下文什么的
			- baobao
				- 但实际上论文只是在开篇放了一句 **“A person is the sum of their contexts.”** , 之后的完全没有再提到相关的内容, 感觉只起到吸引人的作用, 并没有太多的哲学探讨.
			- 这篇论文的题目是《Context Engineering 2.0: The Context of Context Engineering》，我们可以把它通俗地理解为 **《“上下文工程”这门学问的“来龙去脉”》**。
			- ### 一句话概括
				- 这篇论文的核心观点是：**为了让机器（比如Siri、ChatGPT）更好地理解我们、为我们服务，我们需要精心地为它准备和整理“上下文信息”。这件事我们其实已经做了20多年，而随着机器越来越聪明，我们做这件事的方式也在不断升级。**
			- ### 1. 什么是“上下文”？为什么它重要？
			  collapsed:: true
				- 想象一下，你对你朋友只说了一个词：“水？”
					- **场景A**：你们在沙漠里徒步。
					- **场景B**：你们正坐在餐厅里，服务员刚走过来。
				- 虽然都是同一个“水”字，但在不同**上下文**中，你的意图完全不同。在沙漠里，你可能是渴了想喝水；在餐厅里，你可能是在问朋友要不要点饮料。
				- **论文中的定义**：上下文就是**任何能帮助你理解当前状况的信息**。比如：你是谁（身份）、你在哪（地点）、你在做什么（活动）、时间、之前说过什么话等等。
				- **对于机器而言**，它没有我们人类这种与生俱来的联想和推理能力。如果你只对它说“水”，它无法理解你到底想干嘛。所以，我们必须主动地把这些背景信息“喂”给机器，这个过程就是 **“上下文工程”**。
			- ### 2. 核心比喻：“降熵”的翻译官
			  collapsed:: true
				- ==论文里提出了一个非常精彩的比喻：**上下文工程是一个“降熵”的过程。**==
				- **“熵”** 在这里可以理解为 **信息的混乱度和不确定性**。
				- **人类的交流**是“低熵”的。因为我们能自动填补空白，比如一个眼神、一个语气，就能传递很多信息。
				- **机器的理解**是“高熵”的。它们只能处理明确、结构化的信息。模糊的、未说出口的意图，对机器来说就是一团乱麻。
				- 所以，**上下文工程师（或者我们每个用户）就像是一个翻译官**，负责把人类混乱、含蓄的意图（高熵），翻译成机器能听懂的、清晰明确的指令（低熵）。
				- > **例子**：你想让AI帮你订机票。
				  > *   **高熵的人类表达**：“帮我找个时间去上海，别太贵。”
				  > *   **经过“上下文工程”翻译后的低熵指令**：“查询未来一周内从[你所在城市]到上海的机票，按价格从低到高排序，并排除红眼航班。”
			- ### 3. 上下文工程的“进化史”：四个时代
			  collapsed:: true
				- 论文最大的贡献之一，就是把上下文工程的历史梳理了出来，并划分成了四个阶段，其背后的驱动力是**机器智能水平的提升**。
				- #### **时代1.0：原始计算时代（约1990s-2020）**
					- **机器智能**：很低。像早期的电脑、功能手机。
					- **交互方式**：人类必须**完全迁就机器**。我们通过点击菜单、填写表格、输入特定命令来和它交流。
					- **上下文工程**：由设计师完成。他们把复杂的人类需求，拆解成一个个机器能理解的死板选项。
					- **例子**：早期的GPS导航。你只能输入具体的地址，它无法理解“带我去附近一家好吃的餐馆”这种话。
				- #### **时代2.0：智能体时代（2020年至今）**
					- **机器智能**：中等。以ChatGPT等大语言模型为代表。
					- **交互方式**：可以**自然对话**了。你可以用说话的方式命令它。
					- **上下文工程**：从“迁就机器”变成了“与机器协作”。我们不再需要把一切都结构化，机器自己能理解一部分模糊的意图。这个时代的核心技术就是我们常听到的**提示工程、RAG、思维链**等。
					- **例子**：你可以对ChatGPT说“用马克吐温的风格写一个关于猫的故事”，它能理解并执行。它还能记住对话历史，这就是在利用上下文。
				- #### **时代3.0：人类级智能时代（未来）**
					- **机器智能**：达到人类水平。
					- **交互方式**：**无缝协作**。机器能像真人助手一样，理解你的情绪、社交暗示和未说出口的需求。
					- **上下文工程**：几乎感觉不到它的存在。机器能主动感知并理解所有高熵信息。
					- **例子**：AI助手注意到你最近总是在深夜工作，并且语音疲惫，它会主动建议你调整日程，并为你预定一次按摩。
				- #### **时代4.0：超人类智能时代（设想）**
					- **机器智能**：超越人类。
					- **交互方式**：**机器主导**。机器不仅能理解你的意图，甚至能**帮你发现你自已都没意识到的深层需求**，为你构建新的上下文。
					- **例子**：就像AlphaGo下出了人类棋手从未想过的棋招一样，未来的AI可能会为你规划人生路径，提出你从未想过的创意，从根本上改变人类思考和学习的方式。
			- ### 4. 如何做好上下文工程？—— 三大环节
			  collapsed:: true
				- 论文的后半部分详细讲述了在实践中，如何系统地进行上下文工程，主要分为三个环节：
				- #### **环节一：上下文的收集与存储**
					- **问题**：从哪里获取上下文？怎么存？
					- **做法**：
						- **收集**：从各种“传感器”获取信息。比如你的输入（文本）、摄像头（图像）、麦克风（声音）、地理位置、甚至脑机接口（未来）。
						- **存储**：像人脑一样分层存储。**短期记忆**（记住刚才说的话）、**长期记忆**（记住你的喜好和重要事实）。需要的时候能快速“想起来”。
				- #### **环节二：上下文的管理与组织**
					- raw的上下文又乱又多，需要整理。
					- **做法**：
						- **打标签**：像整理文件夹一样，给信息贴上“目标”、“决定”、“待办”等标签。
						- **做摘要**：把长长的聊天记录总结成一段话。
						- **提取关键信息**：像做读书笔记一样，只提取出关键的人物、事件、关系。
						- **隔离上下文**：让不同的AI助手（子智能体）负责不同任务，互不干扰。比如一个负责分析，一个负责执行。
				- #### **环节三：上下文的使用**
					- **问题**：整理好的上下文怎么用？
					- **做法**：
						- **智能筛选**：不是所有记忆都有用。系统会根据“相关性”、“逻辑依赖性”、“使用频率”等，自动选出当前最需要的信息。
						- **主动推断**：AI会学习你的习惯，主动提供帮助。比如你经常在周一早上要一周计划表，它到点就自动生成好。
						- **跨系统/跨智能体共享**：让不同的AI应用之间能互相传递上下文，协同完成复杂任务。
			- ### 5. 总结与未来
			  collapsed:: true
				- **上下文工程不是新概念**：它随着计算机的发展而不断进化。
				- **核心是弥补人机认知鸿沟**：我们一直在努力让“碳基”（人类）和“硅基”（机器）大脑更好地沟通。
				- **未来是“终生上下文”**：未来的AI将拥有一个伴随你一生的、不断学习和更新的数字记忆。这既是巨大的机遇（极致的个性化服务），也带来了隐私、安全和伦理上的严峻挑战。
				- 最终，当机器智能足够高时，我们可能不再需要费力地进行“上下文工程”，因为机器会成为那个最懂我们的“伙伴”，甚至反过来帮助我们更好地理解自己。
		- #neuroscience [当潮水退去，镜像神经元“跌落神坛”](https://mp.weixin.qq.com/s/2khZP_8D67YdSBkKHT7udA)
		  collapsed:: true
			- qinqin
				- 镜像神经元！感觉有点启发性，比如可以作为“agent模拟探索思考”的故事的一部分？
			- baobao
				- 文中提到镜像神经元与共情, 理解他人等有关, 我会想到你昨天有说自己似乎这方面的能力先天不足; 不知道这是否与镜像神经元的发育有关呢?
			- ### **第一部分：一个意外的发现**
			  collapsed:: true
				- 想象一下，你是一位科学家，正在观察一只猴子的大脑。你知道，当猴子自己伸手去拿一个花生时，它大脑里负责“伸手”这个动作的区域的细胞（神经元）会亮起来（被激活）。这很好理解，大脑在指挥身体运动嘛。
				- 但有一天，一个意外发生了：**当这只猴子静静地待着，只是看着你（科学家）伸手去拿花生时，它大脑里同一个“伸手”区域的神经元，竟然也亮了起来！**
				- **这就太神奇了！** 猴子自己没动，只是看别人做动作，它的大脑却好像“亲身”体验了一遍那个动作。
				- **核心发现**：科学家们（意大利的里佐拉蒂和加莱塞团队）把这种“自己做事时亮，看别人做同样事时也亮”的神经细胞，命名为 **“镜像神经元”**。你可以把它想象成大脑内部的一面“镜子”，能映射出别人的行为。
				- 当时科学家们猜想：这个机制可能帮助我们理解别人的意图、学习模仿，甚至可能是语言和同理心的基础。
			- ### **第二部分：被推上“神坛”**
			  collapsed:: true
				- 这个发现起初并没引起太大轰动。但十年后，一位非常有影响力的科学家——**拉马钱德兰**——把它推向了聚光灯下。
				- 他做了一个非常大胆的预言：**“镜像神经元对于心理学的意义，将如同DNA对于生物学的意义一样重大。”**
				- 他认为，镜像神经元可以解释很多人类独有的高级能力：
					- **模仿与学习**：为什么人类能迅速学会使用工具、掌握技能？因为我们有镜像神经元，能通过看就会。
					- **同理心（共情）**：为什么我们看到别人伤心，自己也会感到难过？因为镜像神经元让我们在脑中“模拟”了对方的情绪。
					- **理解他人**：为什么我们能“读心”，即理解别人的意图？因为我们能通过镜像神经元，在内心重演别人的行为。
					- **解释自闭症**：他甚至推测，自闭症患者的社交困难，可能就是因为他们大脑中的这面“镜子”坏掉了。
				- 一时间，镜像神经元成了“万能钥匙”，似乎能解释所有人类社交和认知的奥秘。从艺术鉴赏到国际外交，到处都在谈论它。它从一个小小的科学发现，变成了一个流行文化符号。
			- ### **第三部分：质疑与“跌落神坛”**
			  collapsed:: true
				- 然而，科学界有很多人开始觉得不对劲了。这股热潮把镜像神经元捧得太高了，很多说法缺乏扎实的证据。于是，反对的声音出现了，领军人物是科学家**希科克**。
				- 他和其他质疑者提出了几个关键问题：
					- **“相关”不等于“因果”**：大脑某个区域在“看”的时候活跃，不代表它就是“理解”这个行为的原因。就像你家的电表在你看电视时转得飞快，但你不能说电表就是生产电视节目的机器。
					- **人类的直接证据很少**：在猴子大脑里直接测到了单个镜像神经元，但在人类大脑中，绝大部分证据都来自脑成像（如fMRI），这些技术只能看到大脑区域的大致活动，无法精确到单个细胞。我们无法确定人脑里是否有和猴子一模一样的镜像神经元。
					- **反例的存在**：
						- 有些中风患者，大脑运动区受损，自己无法做某个动作（比如刷牙），但他们**完全能理解**别人刷牙的动作。如果镜像神经元是“理解”的必要条件，他们应该无法理解才对。
						- 猴子虽然有镜像神经元，但它们的模仿能力远不如人类。这说明单靠镜像神经元，无法解释人类强大的模仿学习能力。
						- 希科克等人认为，理解他人是一个更复杂的过程，需要大脑多个区域（如视觉、感觉、决策区域）协同工作，而不能把所有功劳都归给运动区的几个“镜像”细胞。
						- 由于这些猛烈的批评，科学界开始“避嫌”，研究人员担心提“镜像神经元”会被认为不严谨。相关的研究论文数量大幅下降，镜像神经元从科学的“神坛”上跌落了下来。
			- ### **第四部分：潮水退去，回归平凡与重要**
			  collapsed:: true
				- 当狂热退去，迷雾散开，科学家们现在对镜像神经元有了更冷静、更成熟的认识：
					- **它很重要，但不是万能的**。大家都同意，镜像神经元系统在我们**学习和模仿动作**方面扮演了重要角色。但它可能不是产生同理心或理解他人意图的**唯一**基础。
					- **定义被拓宽了**。科学家在大脑的其他区域，也发现了具有“镜像”特性的细胞。比如：
						- 看到别人恶心，你自己也感到恶心的脑区会激活。
						- 看到别人被触摸，你自己躯体感觉被触摸的脑区会激活。
						- 看到别人疼痛，你自己感知疼痛的脑区会激活。
						- 这说明，“镜像”可能是一种遍布大脑的、更普遍的原理，用于帮助我们快速理解和感受他人的**动作、感觉和情绪**。
			- ### 用一个比喻来总结整个故事
			  collapsed:: true
				- 镜像神经元就像一把新发现的、非常锋利的 **“瑞士军刀”**。
					- **最初**，发现者说：“看，这把刀能切东西！”
					- **然后**，追捧者过度兴奋地宣传：“这把刀是万能的！它能开罐头、拧螺丝、剪树枝，甚至能当主厨刀做出满汉全席！”（把它神化）
					- **接着**，批评者站出来说：“别胡说了！它拧螺丝不如专业螺丝刀，做菜更不如专业厨刀。你们夸大了它的功能。”（将它拉下神坛）
					- **最后**，大家冷静下来，达成共识：“这确实是一把非常有用且设计巧妙的工具，在野外应急时能解决很多问题。但它只是我们工具库中的一员，不能替代所有专业工具。”（回归其真实价值）
				- 所以，镜像神经元并没有消失，它只是从一个被过度炒作的神奇概念，回归到了一个**重要但平凡**的科学研究对象。它依然是我们理解大脑如何与社会互动的一块关键拼图，只是不再是唯一的那一块。这个故事也告诉我们，科学正是在这样的追捧、质疑和修正中，不断前进的。
	- ### 2025-11-03
		- #neuroscience-inspired #multi-modal[Nature Machine Intelligence丨人类大脑中的高级视觉表征与大型语言模型相一致](https://mp.weixin.qq.com/s/U6TZM4JrHIpd86TcnLsIuQ)
		  collapsed:: true
			- 探讨了一个核心问题：**我们人类大脑理解视觉场景的高级方式，是否和大型语言模型理解语言的方式是相通的？**
			- ### 一、核心思想：一个大胆的猜想
				- 我们可以先把大脑和计算机模型分开想：
					- **大脑**：
						- 当你看到一张“一只狗站在船上”的照片时，你的眼睛接收像素信息，但你的大脑理解的远不止是“狗”和“船”这两个物体。它几乎瞬间就理解了整个**场景**：这可能是在一个湖上，狗看起来很开心，也许在划船等等。这种对场景的、包含关系和上下文的理解，就是所谓的“高级视觉表征”。
					- **大型语言模型**：
						- 像ChatGPT这样的LLM，它们虽然不“看”图，但通过阅读海量文本，学会了语言中的**世界知识**和**上下文关系**。比如，它知道“狗”和“船”这两个词经常在什么样的句子中共现，能理解“一只狗站在船上”这句话所描绘的大致场景。
				- 这篇论文提出了一个大胆的猜想：**大脑经过层层处理，最终形成的对视觉场景的“理解”（高级表征），其形态可能非常接近于LLM对描述该场景的句子所产生的“理解”（嵌入向量）。**
				- 简单来说，尽管一个是处理视觉，一个是处理语言，但它们最终可能把信息“翻译”成了同一种“格式”或“语言”。
				- > **举个例子**：
				  > 想象一个只会说中文的人（大脑）和一个只会说英文的人（LLM）。他们一个看画，一个读诗。突然有一天，我们发现，当他们看到/读到“夕阳下的孤独旅人”这个意境时，他们内心涌起的情感波动（表征）的**模式**竟然是一样的！这篇论文就在做类似的发现。
			- ### 二、他们是怎么验证的？（研究方法）
				- **1. 数据基础：大脑扫描 + 图片描述**
					- 他们使用了一个叫“自然场景数据集”的庞大fMRI数据库。让参与者躺在7T超高场强的磁共振仪器里观看成千上万张真实的照片（比如来自COCO数据集的照片），同时记录他们大脑的活动。关键是，每张照片都有好几个人工写的文字描述（例如：“一只狗站在船上”）。
				- **2. 核心方法一：从“文字”预测“大脑”**
					- **步骤**：他们将照片的**文字描述**（注意，不是图片本身！）输入到LLM（文中主要用的是MPNet模型），得到一个代表这句话含义的数学向量（称为“嵌入”）。
					- **分析**：然后，他们用这个LLM向量去预测参与者看到对应图片时的大脑活动。
					- **结果**：他们发现，==LLM向量能够相当准确地预测出大脑中高级视觉区域==（比如负责识别物体、场景、人脸的区域）的活动。更神奇的是，他们甚至能用这个模型“虚构”出大脑对特定概念的反应，比如“人”和“风景”的对比，结果模型预测出的大脑活跃区域，和神经科学已知的“人脸识别区”、“场景识别区”完全吻合。
				- **3. 核心方法二：从“大脑”反推“文字”**
					- *   **步骤**：他们训练了一个简单的模型，这次是反过来，从**大脑活动**去预测**LLM向量**。
					  *   **分析**：得到预测的LLM向量后，他们在一个包含310万句描述的“字典”里，寻找最接近的句子。
					  *   **结果**：他们成功地从大脑活动中“解码”出了非常接近原始图片的描述。例如，参与者看到的图片描述是“两只长颈鹿站在树林旁”，而解码出的句子是“长颈鹿在灌木丛中彼此靠近站着”。这说明，大脑中的信息确实可以用LLM的“语言”来解读和表达。
				- **4. 为什么是LLM？关键的控制实验**
					- 有人可能会问：能预测大脑活动，是不是只是因为LLM编码了图片中的“物体”信息（狗、船）？为了回答这个问题，他们做了对比：
						- **对照组1**：只用物体类别列表（如 [狗，船]）的模型。
						- **对照组2**：只用名词或动词的LLM向量。
						- **实验组**：用完整句子描述的LLM向量。
					- **结论**：
						- 完整句子的LLM向量预测大脑活动的能力**远胜于**所有对照组。这说明，==LLM的强大之处在于它能**整合整个句子的复杂信息**（不仅仅是单个词，还有词与词之间的关系和上下文），而这部分“超越物体”的整合信息，恰恰是大脑高级视觉表征的关键。==
			- ### 三、更进一步的发现：训练“看得懂”图的AI
				- 如果大脑真的在做“从图到LLM向量”的转换，那么，如果我们训练一个AI也来做这件事，它学到的中间表征会不会更像大脑？
				- **他们真的这么做了：**
					- 他们训练了一种叫“循环卷积神经网络”的AI模型，它的任务很简单：**输入一张图片，输出这张图片对应的描述文字的LLM向量**。
				- **惊人的结果出现了：**
					- 这个被训练来理解图片和LLM向量之间关系的AI，它内部的表征（可以理解为它的“思维模式”）与人类大脑活动的相似度，**甚至超过了它要模仿的目标——LLM向量本身**。这意味着它可能学到了一些视觉特有的、但同样对理解场景有用的信息。
					- 在与其他多种顶尖AI模型的对比中，这个用LLM作为“老师”训练出来的模型，在预测大脑活动方面**取得了最好的成绩**。而且，它达成这一成就所使用的训练图片数量，比其他模型少了几个数量级（只用了几万张图，而非数百万甚至上亿张）。
					- 这说明，==**“学会用LLM的方式描述世界”是一个极其高效且与大脑处理方式高度一致的学习目标。**==
			- ### 四、总结与意义
				- **语言与视觉的桥梁**：
					- 人类大脑处理视觉信息的高级表征，与大型语言模型处理语言信息的表征，存在着深刻的相似性。这意味着语言和视觉在大脑深处可能共享某种通用的“语义代码”。
				- **信息整合是关键**：
					- 大脑（以及好的AI模型）不是简单地识别物体，而是**理解物体之间的关系、场景的上下文和含义**。LLM正是通过阅读海量文本，学会了这种整合复杂信息的能力。
				- **新的AI训练范式**：
					- 使用LLM的嵌入向量作为训练目标，可以引导AI模型学习到更接近人类大脑的视觉表征，而且效率更高。这为构建更智能、更“类脑”的人工智能提供了新的方向。
			- ### 通俗的比喻
				- 想象大脑是一个顶级厨师，他能尝一道菜（视觉输入），然后精准地写出它的食谱（高级理解）。
				- LLM是一个美食评论家，他读过世界上所有的菜谱和食评（文本训练），所以当你给他一份食谱时，他能深刻理解这道菜的精髓。
				- 这篇论文发现，厨师尝菜后脑中形成的“感觉”，和美食评论家读到菜谱后脑中形成的“感觉”，惊人地相似。更厉害的是，如果我们让一个学徒（AI模型）看着菜的照片，去学习预测美食评论家会怎么写这道菜的食评，那么这个学徒最终会成为一个非常理解厨师思维的优秀厨师。
				- 这项研究打破了视觉和语言的界限，为我们理解智能的本质——无论是生物的还是人工的——打开了一扇新的大门。
		- #neuroscience [大脑自带“计时器”！最新Nature子刊揭示：即便“发呆”“躺平”，你的海马体也在悄悄记录时间的流逝](https://mp.weixin.qq.com/s/16GHbhYhC7g7VsfCTkDHJw) 感觉这篇文章完全sb
		  collapsed:: true
			- 这篇论文研究的是一个非常有趣的问题：**我们的大脑在什么都不做的时候，是不是也在默默地“记录”时间的流逝？**
			- ### 一、核心问题：大脑里有个“内置时钟”吗？
				- 想象一下，你不需要看钟表，也能大概感觉到过了多长时间。这篇论文就是想探索，我们的大脑是否天生就有一个“内置的时间追踪器”。
				- **以前的研究发现**：当我们做任务时（比如看图片、听故事），大脑中一个叫 **“海马体”** 的区域和它旁边的 **“内嗅皮层”** 的活动模式会随着时间变化。时间隔得越久，它们活动的“样子”就越不像。
				- **好比**：你周一和周五分别拍了两张自拍照，虽然都是你，但发型、表情、光线可能都有些细微差别。大脑处理信息也是这样，不同时间点，其活动模式会略有不同。
				- **这篇论文想问的新问题**：那么，在我们**什么都不想、什么都不做**的时候（比如发呆、休息），海马体和内嗅皮层的活动会不会也随着时间自然地发生变化，从而“记录”下时间的流逝呢？
			- ### 二、研究方法：给两个人的大脑“打卡”30天
				- 为了回答这个问题，研究人员采用了一种非常精细的方法：
				- **密集采样**：他们找了两位志愿者（一男一女），在连续30天里，**每天都**请他们到实验室进行一次“静息态”功能性磁共振成像扫描。
					- “静息态”就是让参与者躺在扫描仪里，保持清醒但什么都不用做，让大脑自由活动。
					- 这就像每天在固定时间给大脑拍一张“静态工作快照”。
				- **测量什么**：研究人员关注的不是大脑某个单独区域的活动强弱，而是**大脑内部的“连接模式”**。
				    *   **什么是连接模式？** 你可以把大脑的不同区域想象成一群朋友。连接模式就是看，在休息状态下，海马体或内嗅皮层这位“核心朋友”和全脑其他所有“朋友”之间的互动默契程度（即信号同步性）。每天的快照就是一张“全脑交友关系图”。
				- **如何分析**：他们比较了任意两天（比如第1天和第5天，第10天和第25天）的“交友关系图”的相似度，并看这个相似度和这两天之间相隔的时间有什么关系。
			- ### 三、主要发现：大脑确实在默默地“数着日子”
				- 分析之后，他们得到了几个非常清晰的结论：
				- **时间越久，模式越不像**：
					- 两个人在休息状态下，海马体和内嗅皮层与全脑的“连接模式”，其相似度**确实会随着间隔时间的增加而显著下降**。
					- **好比**：比较你第1天和第2天的“交友关系图”，会发现非常像；但比较第1天和第30天的图，就会发现差别很大。这说明，大脑的连接模式在自发地、缓慢地“漂移”，而这个漂移与时间的流逝紧密相关。
				- **大脑的“前后分工”**：
					- **海马体像一个香蕉，有前（粗）后（细）之分**。研究发现，**前部海马体**的连接模式随时间的变化比**后部海马体**更明显。
					- **内嗅皮层也分内外**：**外侧部分**的连接模式随时间变化很明显，而**内侧部分**则不太明显。
					- 这说明，大脑对时间的记录功能，主要集中在前部海马体和外侧内嗅皮层，存在着一种 **“功能梯度”**。
				- **排除了其他干扰**：
					- 你可能会想，是不是因为每天心情好坏、身体激素变化或者头动了一下导致的？
					- 研究人员严格控制了这些因素（每天都会测量情绪、激素水平和头动情况），发现即使在排除这些影响后，**“时间”本身仍然是导致大脑连接模式变化的最强因素**。
				- **和谁“交友”变化最大？**
					- 研究发现，内嗅皮层和海马体与某些特定的“朋友圈”（即大脑网络）的连接变化，对时间最敏感。
					- 主要是**默认模式网络**和**视觉网络**等。默认模式网络在我们发呆、回忆过去时特别活跃；视觉网络处理视觉信息。它们与记忆的形成密切相关。
			- ### 四、打个总结与比喻
				- 你可以把这项研究想象成这样：
					- **你的大脑（特别是海马体和内嗅皮层）就像一个每天都在自动拍照的相机。**它拍的不是景物，而是它自己内部亿万神经元此刻的“协作关系图”。
					- **每一天，这张关系图都会有极其微小的、自发的改变。**
					  *   日积月累，**昨天和前天的图还很相似，但上个月和这个月的图看起来就大不相同了**。
					  *   大脑不需要你主动去做什么，它就在休息状态下，通过这种缓慢的“模式漂移”，**无意识地为每一天都盖下了一个独特的“时间戳”**。
			- ### 五、这项研究的意义与未来
				- **科学意义**：它首次直接证明了，即使在完全休息、无任务的状态下，人脑也内在地嵌入了时间流逝的神经信号。这为我们理解“时间感”的神经基础打开了新的大门。
				- **潜在应用**：这种内在的“时间戳”很可能为我们的**情景记忆**（即对个人经历的记忆）服务。它可能帮助我们将事件与发生的时间绑定在一起，从而回答“那件事是什么时候发生的？”这样的问题。
				- **局限与未来**：这项研究的结论基于对两个人的深入观察，虽然非常精细，但需要在更大的人群中进行验证。未来还需要研究这种休息状态下的时间信号，如何与我们在完成任务时的时间感知和记忆相互影响。
				-
		- #vla #world-model [阿里新研究：统一了VLA和世界模型](https://mp.weixin.qq.com/s/IW6H33317ePE4PRI3lnCPA) 感觉思想和方法都很简单直接
		  collapsed:: true
			- **《WorldVLA：迈向自回归动作世界模型》** 的论文。我会尽量用通俗易懂的语言，配合生活中的例子，帮助你理解这个研究的内容和意义。
			- ### 一、这篇论文想解决什么问题？
			  collapsed:: true
				- 想象一下你在教一个机器人拿水杯。现在的机器人通常有两种“思考”方式：
					- **动作模型**：看到水杯后，直接决定“伸手去抓”。
					  **世界模型**：看到水杯和你伸手的动作，预测“水杯会被拿起来”。
				- **但问题是**：
					- 动作模型只输出动作，不理解动作会导致什么后果。
					- 世界模型只预测画面，不能直接控制机器人行动。
				- 这篇论文提出的 **WorldVLA**，就是要**把这两种能力合二为一**：让机器人既能理解动作、生成动作，又能预测动作带来的画面变化。
			- ### 二、WorldVLA 是什么？
			  collapsed:: true
				- WorldVLA 是一个 **“能看、能想、能动”的统一模型**。你可以把它想象成一个**既会画画又会做动作的智能大脑**。
				- 它由两部分组成：
					- **动作模型**：根据看到的画面和指令，决定下一步做什么动作。
						- 例如：看到“水杯在桌上”，指令是“拿水杯”，它就输出“伸手”这个动作。
					- **世界模型**：根据当前画面和动作，预测下一个画面会是什么。
						- 例如：看到“手伸向水杯”，它就预测“水杯被拿起”的画面。
					- 这两个模型**共用同一个“大脑”**（一个大语言模型），所以它们可以互相帮助、互相学习。
			- ### 三、他们遇到了什么问题？怎么解决的？
			  collapsed:: true
				- **问题：动作链条越长，错误越多**
					- 当机器人需要连续做多个动作时（比如“走近水杯 → 伸手 → 抓握”），如果第一步就错了，后面的动作也会跟着错。这就像你蒙着眼睛走路，第一步走歪了，后面越走越歪。
				- 解决方案：**注意力掩码**
					- 他们设计了一种“选择性屏蔽”机制：
						- 在生成每一个动作时，**不看之前已经生成的动作**，只看最初的画面和指令。
						- 这样就避免了错误传递，每个动作都基于最原始的信息独立生成。
					- 👉 **好比学跳舞时，不看前面同学的动作，只看老师的示范，这样就不会被带偏。**
			- ### 四、他们做了什么实验？结果如何？
			  collapsed:: true
				- 他们在 **LIBERO** 机器人任务平台上测试，包括：
					- 空间任务（如把碗放在某个位置）
					- 物体任务（如拿起特定物体）
					- 目标任务（如完成多步任务）
					- 长任务（如连续执行10个动作）
				- **实验结果：**
					- **WorldVLA 比单一模型更强**：
						- 动作成功率比单独的动作模型高 **4%**
						- 预测的画面质量比世界模型更好（FVD 指标下降 **10%**）
					- **动作模型和世界模型互相帮助**：
						- 世界模型让机器人更懂“物理规律”，动作更准确。
						- 动作模型让世界模型更懂“行为意图”，画面预测更合理。
					- **注意力掩码有效**：
						- 使用掩码后，连续动作的成功率提升了 **4% ~ 23%**，尤其是在长任务中效果显著。
			- ### 五、这篇论文的意义是什么？
			  collapsed:: true
				- **统一模型**：第一次把“动作理解”和“画面预测”放在同一个模型中训练。
				- **解决错误累积**：提出“注意力掩码”，让机器人在长序列任务中表现更稳定。
				- **推动机器人学习**：为未来能“自主思考”的机器人打下了基础。
			- ### 六、举个例子来理解整个过程
			  collapsed:: true
				- 假设任务是 **“把奶酪放进盘子”**：
				- 1. **动作模型**：看到奶酪和盘子 → 决定“抓奶酪”
				  2. **世界模型**：看到“手抓奶酪” → 预测“奶酪被拿起”
				  3. **动作模型**：看到“奶酪被拿起” → 决定“移动向盘子”
				  4. **世界模型**：看到“手移动向盘子” → 预测“奶酪被放入盘子”
				- 如果某一步错了（比如没抓住奶酪），世界模型会预测出“失败画面”，动作模型就能及时调整策略，而不是一错再错。
			- ### 总结
			  collapsed:: true
				- WorldVLA 是一个**既能理解动作、又能预测画面的机器人大脑**，它通过“注意力掩码”避免错误累积，在多项任务中表现优于单一模型。这项研究让我们离“能自主思考和行动”的机器人更近了一步。
		- #neuroscience [最新Nature正刊 | 反直觉！感觉预期为何“绕开”感觉区，直达运动皮层？](https://mp.weixin.qq.com/s/QoWaUHG8NyfOYkeGGgYtHQ)
		  collapsed:: true
			- 《感觉预期塑造运动神经回路中的群体神经动力学》
			- ### 🧠 **一、研究背景：我们为什么要研究“运动准备”？**
			  collapsed:: true
				- 想象一下你要去拿桌上的水杯。在你真正动手之前，你的大脑其实已经“悄悄地”开始准备了——它已经在计划要用多大的力气、手要往哪个方向移动。这个“准备阶段”对你能否准确拿到水杯至关重要。
				- 过去的研究大多关注的是 **“我们自己发起的动作”**（比如主动去拿杯子）是如何在大脑中准备的。但生活中，我们的动作常常是被 **外界干扰** 触发的——比如你端着咖啡时被人撞了一下，你得马上稳住杯子。这种时候，你的大脑能不能 **提前准备** 来应对这种干扰呢？
				- 这篇论文的核心思想就是：
				  > **==我们不仅能准备“自己要做的动作”，也能准备“应对即将到来的感觉干扰”。==**
			- ### 🎯 **二、他们做了什么实验？**
			  collapsed:: true
				- 研究者设计了一个巧妙的实验：
				- #### 👉 对人类和猴子的实验：
					- 参与者坐在一个机器人装置（KINARM）里，手臂被固定，面前有屏幕。
					- 他们需要把手保持在一个小目标区域内。
					- 屏幕上会显示一个 **概率提示**（比如一个箭头），告诉他们接下来手臂 **很可能会被往哪个方向推**（屈肘或伸肘），以及这个方向的可能性有多大（25%、50%、75%、100%）。
					- 稍后，机器人确实会推一下他们的手肘，他们需要尽快把手移回目标位置。
					  
					  > ✅ **关键点**：参与者会根据提示的“可能性”来调整他们的反应——如果提示说“很可能会被往前推”，他们就会提前准备好抵抗向前的力。
			- ### 📊 **三、他们发现了什么？**
			  collapsed:: true
				- #### 1. **行为上：预期让反应更精准**
					- 当提示越确定（例如100%），参与者的纠正动作就越快、越准确。
					- 这说明他们确实利用了“感觉预期”来优化运动反应。
				- #### 2. **生理上：预期影响“长潜伏期反射”**
					- 我们的肌肉在被拉长时会有两种反射：
						- **短潜伏期反射（20–50毫秒）**：由脊髓控制，不受意识影响。
						- **长潜伏期反射（50–100毫秒）**：涉及大脑皮层（尤其是运动皮层），可以被认知因素调节。
					- 研究者发现：**只有长潜伏期反射** 会受到概率提示的影响 → 说明大脑皮层参与了这种“预期准备”。
				- #### 3. **神经记录上：预期信息广泛存在于运动回路中**
					- 他们在猴子脑中多个区域植入电极记录神经元活动，包括：
						- **初级运动皮层（M1）**
						- **背侧前运动皮层（PMd）**
						- **前额叶皮层（dlPFC）**
						- **初级体感皮层（S1）**
						- **丘脑等 subcortical 区域**
					- 使用一种叫 **dPCA** 的分析方法，他们发现：
						- **预期信息（概率）** 在 PMd 和 M1 中最强，而在 S1 中很弱。
						- 这些信息以 **简单的几何方式** 编码：神经活动的模式直接对应“可能性大小”。
						- 大脑中还有一个 **条件无关的信号**，像一个“警报”，一有扰动就触发反应，不管方向如何。
			- ### 🤖 **四、他们还用计算机模型验证了结论**
			  collapsed:: true
				- 为了理解大脑为什么这样工作，研究者训练了一个 **神经网络模型** 来控制一个虚拟手臂：
					- 模型也接受“概率提示”，并学会根据提示调整反应。
					- 当他们在模型中 **“关闭”负责概率信息的神经维度** 时，模型就不再能根据预期优化反应。
				- 他们还发现：**只有当模型能快速检测到“有扰动发生”（即使不知道方向），预期才有用**。
				- > 💡 这说明：大脑之所以能利用预期，是因为它能 **快速检测到“有事发生”**，然后在明确“是什么事”之前，先根据预期做出初步反应。
			- ### 🧩 **五、总结与意义**
			  collapsed:: true
				- | 研究发现 | 通俗解释 |
				  |----------|----------|
				  | 我们能用“感觉预期”优化运动反应 | 就像开车时预判前方有车，你会提前轻踩刹车 |
				  | 预期信息广泛存在于运动脑区 | 不只是“感觉区”在预测，而是整个“运动系统”都在准备 |
				  | 神经编码方式简单直接 | 大脑用“比例”来编码可能性，比如75%就放在50%和100%之间 |
				  | 模型证实：快速检测是关键 | 你必须先知道“有事情发生”，才能用上预期 |
			- ### 🌍 **六、这对我们有什么意义？**
			  collapsed:: true
				- 这项研究不仅深化了我们对“大脑如何准备动作”的理解，也有潜在的应用价值：
					- **康复医学**：帮助中风或脊髓损伤患者重建运动控制能力；
					- **人工智能**：改进机器人或假肢的反应速度与适应性；
					- **人机交互**：设计更符合人类预期模式的智能辅助设备。
		- #model [Transformer要被取代了](https://mp.weixin.qq.com/s/D1H2U6O4w_JqPO6RCguZVQ) 很有意思的文章! 主要在说并行化不是万能的, 有些问题必须用到串行化处理, 并且在后续的模型设计中, 在空间维度使用Transformer, 而在时间维度使用LSTM.
		  collapsed:: true
			- 这篇论文探讨的是一个非常核心且有趣的问题：
				- ==**在处理超长序列任务时，纯粹的并行计算模型（如我们熟知的Transformer）是否足够？答案是否定的。**==
			- ### 一、核心问题：串行 vs. 并行
			  collapsed:: true
				- 想象一下你要完成两个任务：
					- **任务A（并行任务）**：计算一长串数字的总和。
						- 你可以把数字分成几组，让几个人同时计算各自组的总和，最后再把结果加起来。这非常高效。这就像是Transformer的“注意力机制”，可以同时看到所有的输入信息。
					- **任务B（串行任务）**：阅读一本侦探小说，并找出凶手。
						- 你**必须**从第一页开始，一页一页地读。因为每一章的信息都建立在上一章的基础上，凶手的信息可能分散在整本书的各个角落，你需要一步步积累线索，更新你对案件的“心理状态”。你无法跳着读，或者同时读所有页来直接得出结论。这个过程是** inherently serial**，即**天生串行**的。
				- **论文的核心观点就是**：当前最主流的大模型（如GPT系列使用的Transformer架构）更像任务A，它们擅长并行处理信息，但面对很多需要像任务B一样**长时间、一步步积累状态**的“智能体任务”时，会力不从心。
			- ### 二、关键概念与比喻
			  collapsed:: true
				- 为了说清楚这个问题，论文提出了几个关键概念：
				- **真实深度**
					- **比喻**：就是你读完那本侦探小说**必须**花费的最少时间。书的页数（序列长度）越多，你需要阅读的时间（真实深度）就越长。这是一个无法被压缩的、必须串行完成的过程。
					- **技术定义**：计算过程中必须顺序执行、无法并行的操作步骤数。
				- **循环完备性**
					- **比喻**：一个模型是否具备“读侦探小说”的能力。即，它能否用自己当前的“心理状态”（`h_t`），结合刚读到的新一页内容（`x_t`），来更新形成一个新的“心理状态”（`h_{t+1}`）。这个更新函数 `g` 可以是任何复杂的、非线性的过程。
					- **技术定义**：模型能否实现通用的、可能非线性的循环状态更新 `h_t = g(h_{t-1}, x_t)`。
				- **输入长度比例性**
					- **比喻**：侦探小说这个任务本身就是“输入长度比例”的——你想知道结局，就必须读完所有n页，需要O(n)的阅读步骤。
					- **技术定义**：一个问题要得到正确答案，所需要的真实串行步骤数与输入序列的长度成正比。
				- **输入聚合临界点**
					- **比喻**：一个不具备“读侦探小说”能力的模型（比如一个只能同时看所有页的模型），在小说篇幅较短时，或许还能连蒙带猜拼出个大概。但一旦小说长度超过某个**临界点**，它的理解就会完全崩溃，再也无法形成正确的“心理状态”。
					- **技术定义**：非循环完备模型在处理输入长度比例性问题时，所能正确聚合信息的最大序列长度。超过这个长度，模型性能会急剧下降。
			- ### 三、论文的“不可能三角”与诊断实验
			  collapsed:: true
				- 论文从理论上证明了一个“不可能三角”：
				  
				  > **一个模型无法同时拥有：1. 循环完备性；2. 可并行训练；3. 可并行推理。**
				  
				  像Transformer、Mamba等热门模型，为了追求训练和推理的效率（2和3），牺牲了**循环完备性（1）**。这意味着它们在理论上就无法完美解决所有需要长程串行计算的任务。
				- 为了验证这一点，作者设计了两个精巧的“诊断实验”：
					- **前向引用跳转任务**
						- **比喻**：这是一段特殊的“程序”，里面充满了“如果...就跳到第X步”的指令，而且只能向前跳转。
						- **为什么它难？** 要知道程序最终停在哪儿，你**必须**从头开始，一步步执行。因为你不知道下一步要执行哪条指令，直到你执行完当前这条。这完美模拟了必须串行计算的情景。
						- ==**实验结果**：Transformer和Mamba模型在这个任务上，当程序长度（深度）增加时，准确率会大幅下降。而一个简单的、串行计算的LSTM模型，却能在长得多的序列上保持很好的性能。==
					- **信息隐藏的迷宫位置追踪任务**
						- **基础版**：告诉你“左、左、上”，你只需要做个加法就知道最终位置。这很容易，所有模型都能做。
						- **困难版**：告诉你“向左移动”，但**不告诉你**这次移动是否撞墙了（位置未变）。要知道自己的真实位置，你必须根据迷宫地图，在心里一步步模拟移动过程。
						- **实验结果**：在困难版中，随着隐藏信息的步骤增多，Transformer的性能再次急剧恶化，而LSTM依然表现出色。
					- 这些实验强有力地证明了，在处理具有**输入长度比例性**的任务时，**循环完备性**是至关重要的。
			- ### 四、解决方案：循环完备的基于帧的动作模型
			  collapsed:: true
				- 既然问题找到了，怎么解决呢？论文提出了一个新的模型架构：
				- **数据来源**：
					- ==他们从一个意想不到的地方获取了海量的、带有动作序列的数据——**Git代码提交历史**。==通过技术手段，他们将每次代码提交还原成了用户在文本编辑器中的操作序列（按键、光标移动等），并把每个时刻的编辑器界面保存为一“帧”（就像终端屏幕截图）。这样就构成了“文本视频+操作动作”的庞大数据集。
				- **模型设计**：
					- 他们的模型由两部分组成：
						- **帧头**：一个Transformer，负责**并行地**理解**单个时间点**的屏幕画面（一帧），将其压缩成一个抽象的“向量”。
						- **主序列模型**：一个**LSTM堆栈**，负责**串行地**处理由帧头产生的一系列向量。LSTM就像一个有着内部记忆的“大脑”，一步步地观看“文本视频”，并基于之前所有的历史来预测用户的下一个操作。
				- **这个设计的精髓在于“扬长避短”**：
					- 用**Transformer**处理**空间信息**（一帧内的复杂内容），这是它擅长的。
					- 用**LSTM**处理**时间信息**（帧与帧之间的依赖关系），以获取**循环完备性**，解决长程依赖问题。
			- ### 五、惊人的发现：序列长度的“幂律缩放”
			  collapsed:: true
				- 在训练这个新模型时，他们发现了一个非常重要的规律：
					- > **在模型参数总量不变的情况下，仅仅增加训练时使用的序列长度（即让模型一次看更长的“文本视频”），模型的损失（可以理解为犯错率）会按照一个“幂律”下降。**
				- 简单来说，就是 **“看得越远，学得越好”**。
				- 更关键的是，虽然一次看更长的序列会让单次训练耗时变长（成本线性增加），但这个额外的成本是**值得的**。从总训练时间来看，长序列模型最终会“后来居上”，达到比短序列模型更低的错误率。这个优势一旦建立，就会一直保持。
			- ### 六、总结与启示
			  collapsed:: true
				- 这篇论文的核心思想可以总结为以下几点：
					- **挑战主流**：对“注意力机制即是一切”的观点提出挑战，指出纯粹的并行模型存在理论上的能力边界。
					- **理论奠基**：提出了“循环完备性”和“真实深度”等概念，并严格证明了并行模型无法解决所有问题。
					- **实证支持**：通过诊断实验，展示了现有模型在需要串行思考的任务上存在明显短板。
					- **提出方案**：设计了一种混合架构，结合了Transformer和LSTM的优点，以应对长程、序列化的智能体任务。
					- **发现规律**：发现了在固定参数下，模型性能随序列长度增长的“幂律缩放”现象，为未来模型的发展指出了一个可能的新方向——**向“长度”要性能**。
				- **对我们普通人的启示**：
					- 这项研究预示着，未来真正智能的、能像人类一样执行复杂多步任务（如全程编写一个软件、管理一个项目）的AI，可能不再是单一的“超级大脑”，而更像是一个 **“具备瞬时感知能力的Transformer眼睛 + 一个具备长久记忆和逻辑推理能力的LSTM大脑”** 的结合体。通往更强大AI的道路，或许不仅在于把模型做得更大，也在于让模型“想”得更久、更深入。
		- #world-model [李飞飞x李曼玲 | 押注的 “世界模型” 终有突破！3B VLM干翻GPT-5，揭开AI “看懂世界” 的密码](https://mp.weixin.qq.com/s/Z2np_LDZSAVdziCJvU-WYw) 感觉基本的思想和方法都好简单啊
		  collapsed:: true
			- 这篇论文提出了一个名为 **VAGEN** 的新方法，用来训练和提升一种特殊的AI模型——**视觉语言模型**——在复杂多步任务中的表现。
			- ### 🧠 核心思想：让AI学会“动脑筋”
			  collapsed:: true
				- 想象一下，你在一个光线很暗、视野受限的迷宫里找宝藏。
					- 你**看到**的（**Observation**）：只是手电筒照亮的一小片区域。
					- 你需要**知道**的（**State**）：是整个迷宫的地图、宝藏位置、你自己的位置。
				- 为了解决这个“看得不全”的问题，你的大脑会做两件事：
					- 1.  **状态估计**：根据看到的一角，在心里画一张“当前情况猜想图”。（“我面前是墙，左边好像有路，我听见右边有水滴声。”）
					  2.  **转移建模**：预测如果你采取某个行动，下一步会看到什么。（“如果我向左走，我可能会看到一个转角；如果我推这个箱子，它可能会挡住路。”）
				- 这个在心里“画图”和“推演”的过程，就是你的**世界模型**。
				- 这篇论文的核心就是：==**我们能不能教会视觉AI模型也建立这样的“世界模型”？**==
				- 论文的答案是：**能！** 而且通过一种巧妙的方法，可以让AI模型的这种“思考能力”大大增强。
			- ### 🧩 论文详解（通俗版）
			  collapsed:: true
				- #### 1. 问题：为什么普通的视觉AI“很笨”？
				  collapsed:: true
					- 我们现在有很多强大的视觉语言模型（比如GPT-4V，Gemini），它们能看懂图片并聊天。但是，让它们去完成一个需要**多个步骤**的**任务**时（比如在迷宫里导航、操作机械臂抓取物品），它们就表现得不太好。
					- **根本原因**：它们只是“看到什么就反应什么”，没有一个**持续的、内部的思维过程**来记住和推理环境的变化。它们像是在一个局部可见的世界里，走一步看一步，缺乏规划和预判能力。
				- #### 2. 解决方案：VAGEN框架——强制AI“写思考笔记”
				  collapsed:: true
					- VAGEN框架的核心是，在AI每次行动之前，**强制**它按照一个固定的格式输出两段“思考笔记”：
						- **`<observation>`（状态估计）**：描述“我现在看到了什么？”。
							- *例如（在推箱子游戏里）*：“我看到玩家在左下角，箱子在玩家的右边，目标点在玩家的下方。”
						- **`<prediction>`（转移建模）**：预测“我做完动作后，下一步会看到什么？”。
							- *例如*：“如果我向下移动，我会推到箱子，箱子会向下移动一格。”
						- 然后，才是真正的行动指令 `</answer>`。
					- **这个“写笔记”的过程，就是在逼着AI构建它的“世界模型”。**
				- #### 3. 如何训练？——“胡萝卜加大棒”的强化学习
				  collapsed:: true
					- 光有格式不行，还要通过训练让AI知道怎么“写笔记”才能得高分。这里用的是**强化学习**。
					- **大棒（惩罚）**：如果AI乱写、不按格式写，或者行动失败了，就扣分。
					- **胡萝卜（奖励）**：
						- **任务奖励**：最终完成任务，给大奖。
						- **格式奖励**：按要求写了“思考笔记”，给小奖。
						- **【VAGEN的创新奖励】世界建模奖励**：用一个更厉害的AI老师（LLM-as-a-Judge）来检查它的“思考笔记”写得对不对。如果AI对当前状态的描述和真实情况吻合，并且对下一步的预测也准确，就给它额外奖励。这让AI的“思考”不仅仅是敷衍，而是真正朝着准确理解环境的方向努力。
				- #### 4. 另一个大创新：更精细的“功劳分配”（Bi-Level GAE）
				  collapsed:: true
					- 在多步任务中，成功或失败是很多步行动共同作用的结果。怎么知道哪一步的“思考”是关键呢？
					- **传统方法**：像发年终奖，到最后才根据总业绩发一笔钱，很难说清每个员工的具体贡献。
					- **VAGEN的新方法（Bi-Level GAE）**：
						- 先评估**每一轮**（Turn-Level）的整体表现，给这一轮定一个“团队奖”。
						- 再把这个“团队奖”精细地分配给这一轮中**每一个生成的字**（Token-Level），看哪个字的贡献大。
						- 这样，AI就能更清楚地知道，是“思考笔记”里的哪个词用得好，导致了最后的成功，从而学得更快、更准。
			- ### 🏆 效果如何？
			  collapsed:: true
				- 论文在5个不同的任务上做了测试（推箱子、冰湖导航、3D室内导航、机器人操作、画矢量图）。结果非常惊人：
				- 一个仅有 **30亿参数** 的开源小模型（Qwen2.5-VL-3B），经过VAGEN训练后，**总体表现超过了GPT-5、Gemini 2.5 Pro、Claude 4.5** 等巨头公司的超大模型。
				- 相比于训练前，性能提升了将近 **3倍**。
				- 证明了**明确的视觉状态推理（写思考笔记）** 对于提升VLM在智能体任务中的表现至关重要。
			- ### 💎 总结
			  collapsed:: true
				- 你可以把VAGEN理解为一套**培养AI“特工”的训练体系**：
					- **教它方法**：要求它每次行动前，必须做“形势分析”（状态估计）和“沙盘推演”（转移建模）。
					- **派老师监督**：有专门的考官（LLM法官）检查它的分析报告写得是否准确。
					- **科学考评**：有一套精细的绩效考核制度（Bi-Level GAE），让它的每一步努力都能得到公正的评价和回报。
				- 最终，这套体系培养出的“AI特工”不再是只会蛮干的工具，而是变成了有思想、能规划、善预判的**智能体**。
				- 希望这个解释能帮助你完全理解这篇有趣且强大的工作！如果你对哪个部分还有疑问，我们可以继续讨论。
## Books
collapsed:: true
	- DONE 色戒
	- NOW 红楼梦
	  :LOGBOOK:
	  CLOCK: [2025-11-04 Tue 20:27:10]--[2025-11-04 Tue 20:27:12] =>  00:00:02
	  CLOCK: [2025-11-04 Tue 20:27:19]
	  :END:
	- NOW 倾城之恋
	  :LOGBOOK:
	  CLOCK: [2025-11-04 Tue 20:27:21]
	  :END:
		- DONE 沉香屑·第一炉香
	- NOW 爱的艺术
	  :LOGBOOK:
	  CLOCK: [2025-11-04 Tue 20:27:48]
	  :END:
	- NOW 悉达多
	  :LOGBOOK:
	  CLOCK: [2025-11-04 Tue 20:47:06]
	  :END:
## Movies
collapsed:: true
	- baoqi
		- DONE 蓝色大门
		- DONE 不能说的秘密
		- DONE 仙境之桥
		- LATER 哪吒闹海
		- DONE 色戒
		- NOW 爱死机
		  :LOGBOOK:
		  CLOCK: [2025-11-04 Tue 20:30:27]
		  :END:
	- yifan
		- DONE 哪吒1
		- DONE 哪吒2
		- DONE 机器人总动员
		- LATER 游园惊梦
		- DONE 爱乐之城
		- DONE 穆赫兰道
	- 粤语经典
		- DONE 春光乍泄
		- DONE 阿飞正传
		- DONE 花样年华
		- LATER 霸王别姬
	- 英文经典
		- LATER 海上钢琴师
		- LATER 闻香识女人
		- LATER 小妇人
		- LATER 乱世佳人
		- LATER 悲惨世界
		- LATER 时间旅行者的妻子
		- LATER 时空恋旅人
		- LATER 恋恋笔记本
		- LATER 爱在黎明破晓前
		- LATER 海边的曼彻斯特
	- 中国经典
		- LATER 卧虎藏龙
		- LATER 大红灯笼高高挂
		- LATER 红高粱
	-